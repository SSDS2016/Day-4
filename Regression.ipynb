{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Science Summer School - Split '16\n",
    "======================================\n",
    "# Linear and Logistic Regression\n",
    "\n",
    "Stjepan Begušić\n",
    "\n",
    "version 0.1.1\n",
    "\n",
    "`kernel: Python 2.7`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Though it may seem somewhat dull compared to some of the more modern statistical learning approaches seen within the scope of this summer school, linear and logistic regression are still extremely useful and widely used as a statistical learning method. Consequently, a profound understanding of linear methods is an essential weapon in the arsenal of any modern data professional. In this notebook we will address the practical details and implementaion specifics concerning linear classification and regression methods.\n",
    "\n",
    "Our analysis will rely on implementations of linear methods within the `scikit-learn` package, and will be applied to a dataset containing peer-to-peer lending data on borrowers and investors. Official scikit-learn documentation provides a useful user guide on linear models [[1]](#scikit-linear). The statsmodels package will also be considered for statistical computations and significance testing [[2]](#statsmodels). Instructions on how to install this module (in case it did not come with your Anaconda distribution) can be found here: https://conda.anaconda.org/statsmodels\n",
    "\n",
    "For additional reading, a very transparent and straightforward representation of these topics is given in \"An Introduction to Statistical Learning\" [[3]](#islr), including applications in R. The examples demonstrated in this book implemented in Python can be found in a notebook by J. Warmenhoven [[5]](#islr-python). A more comprehensive coverage can be found in \"Elements of Statistical Learning\" [[4]](#esl). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will use a dataset on peer-to-peer lending, given in `loansData.csv` within your repository,  also available at: https://spark-public.s3.amazonaws.com/dataanalysis/loansData.csv \n",
    "\n",
    "Let us read the data into a pandas DataFrame and inspect the variables and the contents of the dataset. For this purpose, let's call `.info()` on our pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the pd.read_csv() method to parse and read the csv data into a pandas DataFrame\n",
    "# please make sure to name your DataFrame 'loansData'\n",
    "\n",
    "\n",
    "# use the .info() or .describe() methods to inspec the data columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore the contents of the data by using the .head() method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count the occurences of unique values of the Home.Ownership column using the .value_counts() method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the contents of the dataset and convert some data columns to numeric representation if necessary. While preparing data, look out for these issues:\n",
    "\n",
    "* Check for NaN entries, before and after data preparation\n",
    "\n",
    "* Numeric data can be represented and interpreted as strings (due to appended measurement units, percentage signs, etc.)\n",
    "\n",
    "* Incorrect data (if we have the luxury of knowing something about our variables, we can detect values which our out of range, or incorrect in another way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use the dropna() method to drop any NA or NaN entries (rows, not columns!)\n",
    "\n",
    "#by using lambda abstraction (lambda x: f(x)) and the pandas DataFrame .apply() method remove the '%' signs from the Interest.Rate variable\n",
    "\n",
    "#get dummy variables for Home.Ownership and add them to the DataFrame\n",
    "#use the .astype('category') method to represent the column as categorical data\n",
    "#use pd.get_dummies() to get the dummy variables, and pd.concat() to add them to our DataFrame\n",
    "\n",
    "\n",
    "# loansData['Debt.To.Income.Ratio'] = loansData['Debt.To.Income.Ratio'].apply(lambda x: x.replace('%','')).astype(float)\n",
    "\n",
    "# loansData['Loan.Length'] = loansData['Loan.Length'].apply(lambda x: x.replace(' months','')).astype(float)\n",
    "\n",
    "# loansData['FICO'] = loansData['FICO.Range'].apply(lambda x: x.split('-')[1]).astype(float)\n",
    "\n",
    "# loansData = loansData.dropna()\n",
    "\n",
    "# loansData.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the variables in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use .info() to check the contents of the processed data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#use the train_test_split function to split the dataset into training and test, with test_size = 0.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us select all the numeric variables, and the dummy variables created from the Home.Ownership categorical variable and ignore (for now) other categorical variables.\n",
    "\n",
    "Before performing any sort of linear regression the data needs to be checked for (multi)collinearities. Let us check the condition number of the correlation matrix and remove potentially colinear variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all the considered input variables are given here\n",
    "input_variables = ['Amount.Requested',\n",
    "                     'Amount.Funded.By.Investors',\n",
    "                     'Loan.Length',\n",
    "                     'Debt.To.Income.Ratio',\n",
    "                     'Monthly.Income',\n",
    "                     'Open.CREDIT.Lines',\n",
    "                     'Revolving.CREDIT.Balance',\n",
    "                     'Inquiries.in.the.Last.6.Months',\n",
    "                     'FICO',\n",
    "                     'MORTGAGE',\n",
    "                     'OTHER',\n",
    "                     'OWN',\n",
    "                     'RENT']\n",
    "\n",
    "# calculate the condition number of the correlation matrix of the input variables\n",
    "# use np.linalg.cond() and the pandas .corr() methods\n",
    "\n",
    "# inspect the correlation matrix and identify collinearities\n",
    "\n",
    "#define the new input variable list (without the dropped ones) and calculate the condition number of the correlation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use linear regression to answer some important questions which might arise in a real-life setting:\n",
    "* Is there a relationship between the borrower information and the interest rate?\n",
    "* How strong is the relationship?\n",
    "* Which variables influence the interest rate and how string is the effect?\n",
    "* Can we predict (and with what accuracy) the loan interest rate by knowing the information on the borrower?\n",
    "\n",
    "The linear regression model assumes a linear relationship between the input vector $X = [X_1,X_2,...,X_p]^T$ and the output $Y$: \n",
    "\n",
    "$$Y = \\beta_0 + \\sum_{j = 1}^{p}\\beta_jX_j + \\epsilon.$$\n",
    "\n",
    "Here $\\beta_0$ is the intercept term - the expected value of $Y$ when $X = 0$, and $\\beta_j$ is the slope - the average increase in $Y$ associated with a one-unit increase in $X_j$. The error term $\\epsilon$ captures everything the model cannot predict: the true relationship may not be linear, there may be other variables unaccounted for, or there may be measurement or other variation in the data.\n",
    "\n",
    "Typically the coefficients $\\beta = [\\beta_0,\\beta_1,...,\\beta_p]^T$ are estimated to minimize the residual sum of squares (RSS):\n",
    "\n",
    "$$RSS(\\beta) = \\sum_{i = 1}^{N}(y_i - \\hat{y}_i)^2 = (\\mathbf{y}-\\mathbf{X}\\beta)^T(\\mathbf{y}-\\mathbf{X}\\beta),$$\n",
    "\n",
    "where $\\mathbf{X} \\in \\mathbb{R}^{N\\times (p+1)}$ contains the input vectors (with 1 in the first position for the intercept term), and $\\mathbf{y} \\in \\mathbb{R}^{N}$ is the vector of outputs in the training set.\n",
    "\n",
    "Assuming that $\\mathbf{X}$ has full column rank ($\\mathbf{X}^T\\mathbf{X}$ is positive definite), the least squares estimate of the coefficients reads:\n",
    "\n",
    "$$\\hat{\\beta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "The linear regression coefficients can be estimated by using the `linear_model.LinearRegression` class from `scikit-learn` and the `fit()` method. The regression coefficients $\\beta$ can be accessed in the `intercept_` and `coef_` attributes. The predicted values for a set of inputs can be obtained using the `predict()` method. \n",
    "\n",
    "Detailed documentation on these, along with a few useful examples, can be found at:  http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try out the univariate linear regression: select an input variable, initialize the model, fit the model to the data and inspect the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# use the Debt.To.Income.Ratio as the input variable and fit a linear regression model\n",
    "# use the linear_model.LinearRegression class and the .fit() method\n",
    "\n",
    "# access and print the regression coefficients, using the intercept_ and coef_ attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training data and the regression line, using the fitted model predictions. Model predictions are obtained using the .predict() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use .scatter() in matplotlib.pyplot to plot the data and .plot() to add the regression line\n",
    "# to obtain the regression line, you can use the model predictions from the LinearRegression.predict() method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Other simple things to explore:\n",
    "* Calculate and plot residuals\n",
    "* Inspect the residual distribution (probability plot - implemented in scipy.stats.probplot http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.probplot.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient Estimates\n",
    "\n",
    "#### t-test\n",
    "The t-test is used to check the significance of individual regression coefficients. The null hypothesis to test the significance of a particular regression coefficient estimate $\\hat{\\beta_j}$ is $H_0: \\beta_j = 0$, and the alternative $H_1: \\beta_j \\neq 0$. The test statistic is based on the $t$-distribution and is given by:\n",
    "\n",
    "$$t = \\frac{\\beta_j - 0}{SE(\\beta_j)},$$\n",
    "\n",
    "where $SE(\\beta_j)$ is the standard error of the $\\beta_j$ estimate. The t-statistic basically measures the number of standard deviations that $\\beta_j$ is away from 0. If there is no relationship between $X_j$ and $Y$, the t-statistic will have a t-distribution with $N-p-1$ degrees of freedom. In most cases we test whether $\\beta_0 = 0$ (these t-statistics are used to test the significance of the corresponding regressor) - however, when the t-statistic is needed to test the hypothesis of the form $H_0: \\beta = \\beta_0$, then a non-zero $\\beta_0$ may be used. The probability of observing any value equal or greater than $|t|$ is the p-value.\n",
    "\n",
    "The scikit-learn module does not calculate this information, but there are other ways to retrieve it:\n",
    "* write a piece of code to calculate it when necessary\n",
    "* extend the LinearRegression class to do this\n",
    "* use another package, like scipy or statsmodels\n",
    "\n",
    "Let us use the statmodels implementation of the ordinary least squares to perform the regression and inspect the significance of the coefficient estimates. The documentation can be found here:\n",
    "http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLS.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "#use the statsmodels class OLS and the .fit() method\n",
    "#statsmodels does not automatically include an intercept term in the linear regression model\n",
    "#use sm.add_constant() to append a constant term\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy\n",
    "We can measure the model accuracy by using several different methods:\n",
    "\n",
    "#### RSE\n",
    "The residual standard error (RSE) is an estimate of the standard deviation, and is given by:\n",
    "\n",
    "$$RSE = \\sqrt{\\frac{1}{N-p-1}RSS}$$\n",
    "\n",
    "We can calculate the percentage error of the model as the quotient of the RSE by the output mean value: $\\frac{RSE}{\\hat{Y}}$.\n",
    "\n",
    "Note that due to the fact that $RSE$ accounts for $p$, models with more variables can have higher RSE if the decrease in RSS is small relative to the increase in $p$.\n",
    "\n",
    "#### $R^2$ \n",
    "The $R^2$ measure, as opposed to the RSE, provides a measure of the proportion of variance explained by the model. It always takes on a value between 0 and 1 and is given by:\n",
    "\n",
    "$$R^2 = 1 - \\frac{RSS}{TSS},$$\n",
    "\n",
    "where $TSS = \\sum_{i = 1}^{N}(y_i - \\bar{y})^2$ is the total sum of squares. \n",
    "\n",
    "\n",
    "\n",
    "Let us inspect the accuracy of our univariate regression model, use the .score() method to calculate the $R^2$ value, for both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use the .predict() method to calculate residuals\n",
    "\n",
    "\n",
    "#calculate the RSS, use np.sum()\n",
    "\n",
    "\n",
    "#calculate the RSE, using np.sqrt() and len(X) (for the number of observations)\n",
    "\n",
    "\n",
    "#using RSE and the output mean (using np.mean), calculate the percentage error\n",
    "\n",
    "\n",
    "#using the LinearRegression.score() method, calculate the R^2 score for training and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations:\n",
    "\n",
    "* How do you comment the fact that this model has such a good p-value for the coefficient estimate t-test, but a very low $R^2$ score?\n",
    "\n",
    "* Can the $R^2$ score for test set be better than the one for the training set?\n",
    "\n",
    "* Can the $R^2$ score be negative?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now fit a **multivariate linear regression** model using multiple input variables. Estimate the model accuracy and compare it to the univariate case. Is this model better? \n",
    "\n",
    "\n",
    "A very important tool in assessing multivariate regression models is the F-test.\n",
    "\n",
    "\n",
    "### F-test\n",
    "The F-test is used to check the significance of multiple regression coefficients. The null hypothsis reads: $H_0: \\beta_1,\\beta_1,...\\beta_p = 0$ and the alternative is $H_1:$ at least one $\\beta_j$ is non-zero. The test statistic is given by:\n",
    "\n",
    "$$F = \\frac{(TSS-RSS)/p}{RSS/(N-p-1)}.$$\n",
    " \n",
    "The F-test is also included in the OLS class within the statsmodels package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# just a reminder on our input_variables\n",
    "input_variables = ['Amount.Requested',\n",
    "                     'Loan.Length',\n",
    "                     'Debt.To.Income.Ratio',\n",
    "                     'Monthly.Income',\n",
    "                     'Open.CREDIT.Lines',\n",
    "                     'Revolving.CREDIT.Balance',\n",
    "                     'Inquiries.in.the.Last.6.Months',\n",
    "                     'FICO',\n",
    "                     'MORTGAGE',\n",
    "                     'OTHER',\n",
    "                     'OWN']\n",
    "\n",
    "#use the LinearRegression class to fit a multivariate linear regression model\n",
    "\n",
    "# calculate the R^2 score for both the training and test sets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Including additional predictions seems to have improved the model - let us check the F-statistic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# use the statsmodels OLS class to fit the multivariate linear regression model and calculate the F-statistic\n",
    "# don't forget to add a constant term to the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the model has improved by including additional input variables. However, we are almost certain that some of these are more useful for prediction than the others. The scikit-learn package includes several procedures for feature selection. Let us use the SelectKBest function to uncover $K$ individually most significant features. The documentation can be found at: http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "#use the SelectKBest() class to select a couple of most important regression variables and print the selected column names\n",
    "\n",
    "\n",
    "#use the .get_support() method to access the mask (boolean indices) of the selected features\n",
    "\n",
    "\n",
    "#fit the reduced model and calculate the R^2 score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "* Use multivariate linear regression to explore possible interactions between variables of the dataset, such as products or quotients.\n",
    "\n",
    "* Include basis expansions of the input variables in the regression - maybe some of the relationships are not linear?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Regularized regression models\n",
    "\n",
    "The regularized least squares methods are used to either avoid ill-posed least squares problems when the number of variables is greater than the number of observations, or to avoid overfitting and aid the generalization power of the model. All of the regularization techniques used below include minimizing a linear combination of the RSS and the estimated coefficient magnitudes:\n",
    "* **Ridge regression** uses L2 norm of the coefficient magnitudes\n",
    "* **Lasso regression** uses L1 norm of the coefficient magnitudes\n",
    "* **Elasticnet** uses a combination of the L1 and L2 norms\n",
    "\n",
    "Since all of these methods include coefficient magnitudes in the penalty function, the parameters and consequently the results will be considerably dependent on the magnitudes of the input variables. This is why it is always recommended to adjust the input variable magnitues by standardizing or normalizing the input.\n",
    "\n",
    "\n",
    "### Ridge regression\n",
    "By adding the L2 norm of the coefficient magnitudes to the minimization function, ridge regression avoids overfitting while keeping all of the predictor variables included. The penalty function reads:\n",
    "\n",
    "$$\\underset{\\beta}{min\\,} {{|| \\beta X  - y||_2^2} + \\alpha {||\\beta||_2^2}}.$$\n",
    "\n",
    "Due to the added term, with increased value of $\\alpha$ the coefficient magnitudes will decrease. Basicall, the model will rely less on a small set of large predictors (due to large corresponding coefficients) and will tend to distribute these weights accors all variables - thus reducing the possibility of overfitting due to sampling flaws.\n",
    "\n",
    "Ridge regression can be estimated using the sklearn.linear_model.Ridge class the .fit() method. The documentation can be found at: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "\n",
    "\n",
    "### Lasso regression\n",
    "Instead of the L2 norm of the coefficient magnitudes, lasso regression uses the L1 prior as a regularizer. The objective function to minimize is:\n",
    "\n",
    "$$\\underset{\\beta}{min\\,} { \\frac{1}{2N} ||\\beta X - y||_2 ^ 2 + \\alpha ||\\beta||_1}.$$\n",
    "\n",
    "Due to the L1 norm, instead of evenly distributing the coefficients among all the predictor variables, Lasso results in sparse estimates of the regression coefficients - the model tends to set most coefficients to zero, and the other will be assigned high magnitues. Due to this, and inheret variable selection is performed. This is especially useful in cases when the number of variables is larger than the number of observations and the ordinary least squares problem is ill-posed.\n",
    "\n",
    "Lasso regression is given in the sklearn.linear_model.Lasso class and its parameters can be estimated by calling the .fit() method. The documentation can be found at: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html  \n",
    "\n",
    "\n",
    "### ElasticNet\n",
    "The ElasticNet regression model is trained with both L1 and L2 as regularizers. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\n",
    "\n",
    "Elastic-net is useful when there are multiple correlated features - Lasso is likely to pick one of these at random, while elastic-net is likely to pick both. The objective function to minimize is in this case:\n",
    "\n",
    "$$\\underset{\\beta}{min\\,} { \\frac{1}{2N} ||\\beta X - y||_2 ^ 2 + \\alpha \\rho ||\\beta||_1 +\n",
    "\\frac{\\alpha(1-\\rho)}{2} ||\\beta||_2 ^ 2}.$$\n",
    "\n",
    "ElasticNet regression is given in the sklearn.linear_model.ElasticNet class and its parameters can be estimated by calling the .fit() method. The documentation can be found at: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Let us import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us emulate a situation where the number of variables is very high compared to the number of observations in the training set. We will include all of the categorical variables previously ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#first include all the previously ignored predictor variables - the dummy variables for State and Loan.Purpose\n",
    "\n",
    "# loansData.loc[:,'State'] = loansData.loc[:,'State'].astype('category')\n",
    "# dummies = pd.get_dummies(loansData['State'])\n",
    "# loansData = pd.concat([loansData, dummies], axis=1)\n",
    "\n",
    "# loansData.loc[:,'Loan.Purpose'] = loansData.loc[:,'Loan.Purpose'].astype('category')\n",
    "# dummies = pd.get_dummies(loansData['Loan.Purpose'])\n",
    "# loansData = pd.concat([loansData, dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#let us split the data into training and test sets, in such a way that N < p\n",
    "\n",
    "# train, test = train_test_split(loansData, test_size = 0.98,random_state=1950)\n",
    "# input_variables = train.select_dtypes(include=['float64','int64']).drop('Interest.Rate', axis=1).columns\n",
    "\n",
    "# X = train[input_variables]\n",
    "# Y = train['Interest.Rate']\n",
    "\n",
    "# print train.shape\n",
    "# print test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge trace:\n",
    "Use the sklearn.linear_model regularized regressions to explore the relationship between the regularization parameter $\\alpha$ and the estimated coefficient values for ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# alphas = np.linspace(0.001, 5, num=1000)\n",
    "# ridge_regr = Ridge(alpha = alphas[0],normalize=True)\n",
    "\n",
    "# coefficients = []\n",
    "# train_performance = []\n",
    "# test_performance = []\n",
    "# for a in alphas:\n",
    "#     ridge_regr.set_params(alpha=a)\n",
    "#     ridge_regr.fit(X,Y)\n",
    "#     coefficients.append(ridge_regr.coef_)\n",
    "#     train_performance.append(ridge_regr.score(X, Y))\n",
    "#     test_performance.append(ridge_regr.score(test[input_variables], test['Interest.Rate']))\n",
    "\n",
    "\n",
    "\n",
    "# figure(num=None, figsize=(15, 6), dpi=200)\n",
    "# h = plt.plot(alphas, coefficients)\n",
    "# plt.xlabel('alpha')\n",
    "# plt.ylabel('coefficients')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# figure(num=None, figsize=(10, 6), dpi=200)\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.semilogx(alphas, train_performance, label='Train')\n",
    "# plt.semilogx(alphas, test_performance, label='Test')\n",
    "# plt.legend(loc='lower left')\n",
    "# plt.ylim([0, 1.2])\n",
    "# plt.xlabel('Regularization parameter')\n",
    "# plt.ylabel('Performance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now repeat this procedure and inspect the relationship between the parameter $\\alpha$ values and the Lasso regression coefficients. Also explore the train and test set model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#copy-paste the code in the previous cell and use the Lasso() class to fit a lasso regularized regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the values of the $\\alpha$ and $\\rho$ parameters and the ElasticNet regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#copy-paste the code in the previous cell and use the ElasticNet() class to fit a lasso regularized regression model\n",
    "#set the value of the l1_ratio to 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing $\\alpha$\n",
    "\n",
    "The parameter $\\alpha$ is usually chosen using K-fold cross validation: we pick the $\\alpha$ with the least overall cross validation error (CVE). This is implemented in scikit-learn within the RidgeCV, LassoCV and ElasticNetCV classes.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "* **Fit a multivariate linear regression model (non-regularized) and inspect the model accuracy**\n",
    "    What is the $R^2$ score on the train data? What is the score on the test data? How do you comment this?\n",
    "\n",
    "\n",
    "* **Fit an L2-regularized regression model (Ridge regression) to the dataset and inspect the model accuracy**\n",
    "    Find a favorable value of the parameter $\\alpha$ (normalize the input variables). What are the $R^2$ scores for the train and test data?\n",
    "    \n",
    "    \n",
    "* **Fit an L1-regularized regression model (Lasso regression) to the dataset and inspect the model accuracy**\n",
    "    Which variables have non-zero coefficients? Do they match the selected coefficients in the earlier example?\n",
    "    \n",
    "    \n",
    "* **Fit an ElasticNet regression model to the dataset and inspect the model accuracy**\n",
    "    What is the accuracy of the model on the train and test datasets?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Variable selection using Lasso\n",
    "\n",
    "Let us turn back to the Lasso L1 regularized regression model. Due to the inherent sparsity of the coefficient estimates, this is an especially convenient method for variable selection. The idea is simple: find an optimal value of $\\alpha$ using cross-validation, and then explore the estimated regression coefficients - those different than 0 correspond to valuable input variables.\n",
    "\n",
    "\n",
    "Let us detect which coefficients our Lasso model chose as significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#use the fitted lasso model and the .coef_ attribute to access the coefficients\n",
    "#print out those column namse whose corresponding coefficients are != 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these coefficients compare to our identified significant coefficients above (when using SelectKBest)? Note that these are completely different models, due to added variables and a significantly reduced training set - nevertheless, some variables seem to be important in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than predicting the interest rate given a set of input parameter values, sometimes it is useful to consider binary outcomes: \"Is it possible to get a loan with an interest rate of 12%, given one's parameter values?\"\n",
    "\n",
    "If we characterize all loans with the interest rate above the threshold of 12% as unaffordable and all others as affordable - the question we face read: \"What is the probability of getting an affordable loan, given one's parameters?\"\n",
    "\n",
    "### Classification via logistic regression\n",
    "Logistic regression is a linear model for classification, also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function.\n",
    "\n",
    "The implementation of logistic regression in scikit-learn includes optional L2 or L1 regularization. As in previous examples, regularization helps avoid overfitting and deals with a large number of variables with insufficient observations. As an optimization problem, binary class L2 penalized logistic regression minimizes the following cost function:\n",
    "$$\\underset{w, c}{min\\,} \\frac{1}{2}w^T w + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) .$$\n",
    "Similarly, L1 regularized logistic regression solves the following optimization problem\n",
    "$$\\underset{w, c}{min\\,} \\|w\\|_1 + C \\sum_{i=1}^n \\log(\\exp(- y_i (X_i^T w + c)) + 1) .$$\n",
    "\n",
    "The parameter $C$ can be set to very high values to simply annul the regularization. More can be found in the documentation: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "A very useful notebook covering the logistic regression model in statsmodels is given here: http://blog.yhat.com/posts/logistic-regression-and-python.html\n",
    "\n",
    "Another notebook, using scikit-learn is available here: http://www.dataschool.io/logistic-regression-in-python-using-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Literature\n",
    "-----------\n",
    "\n",
    "<a name=\"scikit-linear\">[1]</a> scikit-learn documentation - 1.1. Generalized Linear Models, [link](http://scikit-learn.org/stable/modules/linear_model.html)\n",
    "\n",
    "<a name=\"statsmodels\">[2]</a> statsmodels documentation, [link](https://pypi.python.org/pypi/statsmodels)\n",
    "\n",
    "<a name=\"islr\">[3]</a> James, G., Witten, D., Hastie, T., and Tibshirani, R. (2014), An Introduction to Statistical Learning: With Applications in R. Springer Publishing Company, Inc., [link](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "<a name=\"esl\">[4]</a> Hastie, T.; Tibshirani, R. & Friedman, J. (2001), The Elements of Statistical Learning, Springer New York Inc., New York, NY, USA. [link](http://statweb.stanford.edu/~tibs/ElemStatLearn/)\n",
    "\n",
    "<a name=\"scikit-linreg\">[5]</a> ISLR - Python, [link](https://github.com/JWarmenhoven/ISLR-python)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python2]",
   "language": "python",
   "name": "Python [python2]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
